{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbd9f780",
   "metadata": {},
   "source": [
    "this cell imports libraries and sets up basic configuration like tickers and feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "TICKERS = [\"AAPL\", \"MSFT\", \"GOOGL\"]\n",
    "NEWS_TEMPLATE = \"news_with_prices_{ticker}.csv\"\n",
    "SENTIMENT_PATH = \"daily_sentiment.csv\"\n",
    "SEQ_LEN = 60\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"Open\", \"High\", \"Low\", \"Close\", \"Volume\",\n",
    "    \"daily_return\",\n",
    "    \"sentiment_score\", \"sent_lag1\", \"sent_lag2\", \"sent_lag3\",\n",
    "    \"sent_roll3\", \"sent_roll7\"\n",
    "]\n",
    "TARGET_COL = \"next_day_return\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beec1b47",
   "metadata": {},
   "source": [
    "this cell defines a function to load, clean, merge sentiment, and engineer features for a single ticker, then loads all tickers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa26771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_ticker(ticker):\n",
    "    path = NEWS_TEMPLATE.format(ticker=ticker)\n",
    "    df = pd.read_csv(path)\n",
    "    cols_to_drop = [\n",
    "        \"Unnamed: 0\",\n",
    "        \"Article_title\",\n",
    "        \"Url\",\n",
    "        \"Publisher\",\n",
    "        \"Author\",\n",
    "        \"Article\",\n",
    "        \"Lsa_summary\",\n",
    "        \"Luhn_summary\",\n",
    "        \"Lexrank_summary\",\n",
    "        \"Stock_symbol\",\n",
    "        \"price_symbol\",\n",
    "        \"is_exact_match\",\n",
    "        \"Date\",\n",
    "        \"news_date\"\n",
    "    ]\n",
    "    df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "    if \"Textrank_summary\" in df.columns:\n",
    "        df = df.drop(columns=[\"Textrank_summary\"])\n",
    "    df[\"date_price\"] = pd.to_datetime(df[\"date_price\"])\n",
    "    df = df.sort_values(\"date_price\")\n",
    "    df = df.groupby(\"date_price\", as_index=False).agg(\n",
    "        {\n",
    "            \"Open\": \"mean\",\n",
    "            \"High\": \"mean\",\n",
    "            \"Low\": \"mean\",\n",
    "            \"Close\": \"mean\",\n",
    "            \"Volume\": \"mean\"\n",
    "        }\n",
    "    )\n",
    "    df[\"daily_return\"] = df[\"Close\"].pct_change()\n",
    "    df[\"next_day_return\"] = df[\"daily_return\"].shift(-1)\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "    sent = pd.read_csv(SENTIMENT_PATH)\n",
    "    sent = sent[sent[\"Stock_symbol\"] == ticker].copy()\n",
    "    sent = sent[[\"news_date\", \"daily_sentiment\"]]\n",
    "    sent = sent.rename(columns={\"news_date\": \"date_price\", \"daily_sentiment\": \"sentiment_score\"})\n",
    "    sent[\"date_price\"] = pd.to_datetime(sent[\"date_price\"])\n",
    "\n",
    "    df = df.merge(sent, on=\"date_price\", how=\"left\")\n",
    "    df[\"sentiment_score\"] = df[\"sentiment_score\"].ffill().fillna(0.0)\n",
    "    df = df.sort_values(\"date_price\").reset_index(drop=True)\n",
    "\n",
    "    df[\"sent_lag1\"] = df[\"sentiment_score\"].shift(1)\n",
    "    df[\"sent_lag2\"] = df[\"sentiment_score\"].shift(2)\n",
    "    df[\"sent_lag3\"] = df[\"sentiment_score\"].shift(3)\n",
    "    df[\"sent_roll3\"] = df[\"sentiment_score\"].rolling(3).mean()\n",
    "    df[\"sent_roll7\"] = df[\"sentiment_score\"].rolling(7).mean()\n",
    "\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "ticker_dfs = {t: load_and_prepare_ticker(t) for t in TICKERS}\n",
    "ticker_dfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b646d7",
   "metadata": {},
   "source": [
    "this cell defines a function to scale features, create sliding window sequences, and split into train, validation, and test sets for each ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65421894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sequences_from_df(df, seq_len=SEQ_LEN):\n",
    "    features = df[FEATURE_COLS].values\n",
    "    target = df[TARGET_COL].values\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    X_all = scaled_features\n",
    "    y_all = target\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - seq_len):\n",
    "        X.append(X_all[i:i + seq_len])\n",
    "        y.append(y_all[i + seq_len])\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    n = len(X)\n",
    "    train_end = int(n * 0.7)\n",
    "    val_end = int(n * 0.85)\n",
    "    X_train = X[:train_end]\n",
    "    y_train = y[:train_end]\n",
    "    X_val = X[train_end:val_end]\n",
    "    y_val = y[train_end:val_end]\n",
    "    X_test = X[val_end:]\n",
    "    y_test = y[val_end:]\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, scaler\n",
    "\n",
    "seq_data = {}\n",
    "for ticker in TICKERS:\n",
    "    seq_data[ticker] = build_sequences_from_df(ticker_dfs[ticker])\n",
    "    shapes = [arr.shape for arr in seq_data[ticker][:5]]\n",
    "    print(ticker, \"shapes:\", shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc529a",
   "metadata": {},
   "source": [
    "this cell defines helper classes and functions to build the transformer model, the lstm model, and the linear baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    def call(self, x):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        d_model = tf.shape(x)[2]\n",
    "        positions = tf.cast(tf.range(seq_len)[:, tf.newaxis], tf.float32)\n",
    "        dims = tf.cast(tf.range(d_model)[tf.newaxis, :], tf.float32)\n",
    "        angle_rates = 1.0 / tf.pow(10000.0, (2 * (dims // 2)) / tf.cast(d_model, tf.float32))\n",
    "        angle_rads = positions * angle_rates\n",
    "        sines = tf.sin(angle_rads[:, 0::2])\n",
    "        cosines = tf.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return x + pos_encoding\n",
    "\n",
    "def build_transformer(seq_len, n_features):\n",
    "    inputs = layers.Input(shape=(seq_len, n_features))\n",
    "    x = layers.Dense(64)(inputs)\n",
    "    x = PositionalEncoding()(x)\n",
    "    attn = layers.MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    attn = layers.Dropout(0.1)(attn)\n",
    "    x = layers.Add()([x, attn])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    ff = layers.Dense(128, activation=\"relu\")(x)\n",
    "    ff = layers.Dense(64)(ff)\n",
    "    ff = layers.Dropout(0.1)(ff)\n",
    "    x = layers.Add()([x, ff])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "def build_lstm(seq_len, n_features):\n",
    "    inputs = layers.Input(shape=(seq_len, n_features))\n",
    "    x = layers.LSTM(64)(inputs)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "def train_keras_model(model, X_train, y_train, X_val, y_val, epochs=80, batch_size=16):\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True)\n",
    "    rl = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es, rl],\n",
    "        verbose=0\n",
    "    )\n",
    "    return model, history\n",
    "\n",
    "def train_linear_baseline(X_train, y_train):\n",
    "    n, sl, f = X_train.shape\n",
    "    X_flat = X_train.reshape(n, sl * f)\n",
    "    reg = LinearRegression()\n",
    "    reg.fit(X_flat, y_train)\n",
    "    return reg\n",
    "\n",
    "def predict_linear(reg, X):\n",
    "    n, sl, f = X.shape\n",
    "    X_flat = X.reshape(n, sl * f)\n",
    "    return reg.predict(X_flat)\n",
    "\n",
    "def predict_naive_zero(X):\n",
    "    return np.zeros(len(X))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c28a4ae",
   "metadata": {},
   "source": [
    "this cell defines functions for computing metrics and printing a small metrics table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe700801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred) ** 0.5\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    dir_true = (y_true > 0).astype(int)\n",
    "    dir_pred = (y_pred > 0).astype(int)\n",
    "    direction_accuracy = (dir_true == dir_pred).mean()\n",
    "    return {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2\": r2,\n",
    "        \"Corr\": corr,\n",
    "        \"DirAcc\": direction_accuracy\n",
    "    }\n",
    "\n",
    "def print_metrics_table(ticker, metrics_dict):\n",
    "    dfm = pd.DataFrame(metrics_dict).T\n",
    "    dfm[\"DirAcc\"] = dfm[\"DirAcc\"].apply(lambda x: f\"{x*100:.2f}%\")\n",
    "    print(f\"=== {ticker} ===\")\n",
    "    display(dfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb19c499",
   "metadata": {},
   "source": [
    "this cell trains the transformer, lstm, linear, and naive models for each ticker and stores metrics and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264ce92",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, scaler = seq_data[ticker]\n",
    "    n_features = X_train.shape[2]\n",
    "\n",
    "    transformer = build_transformer(SEQ_LEN, n_features)\n",
    "    transformer, _ = train_keras_model(transformer, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    lstm = build_lstm(SEQ_LEN, n_features)\n",
    "    lstm, _ = train_keras_model(lstm, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    lin_reg = train_linear_baseline(X_train, y_train)\n",
    "\n",
    "    y_pred_trans = transformer.predict(X_test).flatten()\n",
    "    y_pred_lstm = lstm.predict(X_test).flatten()\n",
    "    y_pred_lin = predict_linear(lin_reg, X_test)\n",
    "    y_pred_naive = predict_naive_zero(X_test)\n",
    "\n",
    "    metrics = {\n",
    "        \"NaiveZero\": evaluate_predictions(y_test, y_pred_naive),\n",
    "        \"Linear\": evaluate_predictions(y_test, y_pred_lin),\n",
    "        \"LSTM\": evaluate_predictions(y_test, y_pred_lstm),\n",
    "        \"Transformer\": evaluate_predictions(y_test, y_pred_trans),\n",
    "    }\n",
    "    results[ticker] = {\n",
    "        \"metrics\": metrics,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_pred_trans\": y_pred_trans,\n",
    "        \"X_test\": X_test,\n",
    "        \"transformer\": transformer\n",
    "    }\n",
    "    print_metrics_table(ticker, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679a7c21",
   "metadata": {},
   "source": [
    "this cell runs a randomization test where the training targets are shuffled to check if the transformer performs similarly on noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345fc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomization_test_transformer(X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    y_train_shuffled = np.random.permutation(y_train)\n",
    "    n_features = X_train.shape[2]\n",
    "    model = build_transformer(SEQ_LEN, n_features)\n",
    "    model, _ = train_keras_model(model, X_train, y_train_shuffled, X_val, y_val, epochs=60)\n",
    "    y_pred = model.predict(X_test).flatten()\n",
    "    return evaluate_predictions(y_test, y_pred)\n",
    "\n",
    "randomized_results = {}\n",
    "for ticker in TICKERS:\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test, _ = seq_data[ticker]\n",
    "    randomized_results[ticker] = randomization_test_transformer(\n",
    "        X_train, y_train, X_val, y_val, X_test, y_test\n",
    "    )\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    print(f\"=== randomization test: {ticker} ===\")\n",
    "    dfm = pd.DataFrame({\"Randomized_Transformer\": randomized_results[ticker]}, index=randomized_results[ticker].keys()).T\n",
    "    display(dfm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9006244",
   "metadata": {},
   "source": [
    "this cell plots actual versus predicted returns, cumulative returns, and rolling directional accuracy for a chosen ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea41a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_to_plot = \"AAPL\"\n",
    "\n",
    "y_test = results[ticker_to_plot][\"y_test\"]\n",
    "y_pred = results[ticker_to_plot][\"y_pred_trans\"]\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(y_test, label=\"actual\", marker=\"o\")\n",
    "plt.plot(y_pred, label=\"transformer predicted\", marker=\"x\")\n",
    "plt.title(f\"{ticker_to_plot} next-day returns: actual vs predicted\")\n",
    "plt.xlabel(\"test sample\")\n",
    "plt.ylabel(\"return\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "actual_cum = np.cumsum(y_test)\n",
    "pred_cum = np.cumsum(y_pred)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(actual_cum, label=\"actual cumulative return\", linewidth=2)\n",
    "plt.plot(pred_cum, label=\"predicted cumulative return\", linewidth=2)\n",
    "plt.title(f\"{ticker_to_plot} cumulative actual vs predicted returns\")\n",
    "plt.xlabel(\"test sample\")\n",
    "plt.ylabel(\"cumulative return\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "dir_true = (y_test > 0).astype(int)\n",
    "dir_pred = (y_pred > 0).astype(int)\n",
    "rolling_acc = pd.Series((dir_true == dir_pred).astype(int)).rolling(10).mean()\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(rolling_acc, label=\"rolling 10-day direction accuracy\", linewidth=2)\n",
    "plt.axhline(0.5, color=\"gray\", linestyle=\"--\", label=\"50% baseline\")\n",
    "plt.title(f\"{ticker_to_plot} rolling directional accuracy\")\n",
    "plt.xlabel(\"test sample\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d0b5e",
   "metadata": {},
   "source": [
    "this cell computes kernel shap feature importance for a chosen ticker using the transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63736448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_shap_importance_for_ticker(ticker, sample_size=40, background_size=40):\n",
    "    X_test = results[ticker][\"X_test\"]\n",
    "    model = results[ticker][\"transformer\"]\n",
    "    X_train, _, _, _, _, _, _ = seq_data[ticker]\n",
    "\n",
    "    X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "    X_test_flat = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "    background_idx = np.random.choice(len(X_train_flat), min(background_size, len(X_train_flat)), replace=False)\n",
    "    background = X_train_flat[background_idx]\n",
    "\n",
    "    def pred_wrapper(x):\n",
    "        x_seq = x.reshape(-1, SEQ_LEN, X_train.shape[2])\n",
    "        return model.predict(x_seq, verbose=0).flatten()\n",
    "\n",
    "    explainer = shap.KernelExplainer(pred_wrapper, background)\n",
    "\n",
    "    sample_idx = np.random.choice(len(X_test_flat), min(sample_size, len(X_test_flat)), replace=False)\n",
    "    X_sample = X_test_flat[sample_idx]\n",
    "\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    shap_vals = np.array(shap_values).reshape(len(X_sample), SEQ_LEN, X_train.shape[2])\n",
    "    shap_mean = np.mean(np.abs(shap_vals), axis=(0, 1))\n",
    "\n",
    "    feature_names = FEATURE_COLS\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(feature_names, shap_mean)\n",
    "    plt.title(f\"{ticker} kernel shap feature importance\")\n",
    "    plt.xlabel(\"mean |shap|\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "ticker_to_explain = \"AAPL\"\n",
    "kernel_shap_importance_for_ticker(ticker_to_explain)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
