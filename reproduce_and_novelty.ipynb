{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c132af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import requests, zipfile\n",
    "\n",
    "DATA_URL = \"https://github.com/rohanbadami/DS340W/releases/download/dataset/processed.zip\"\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_ROOT = ROOT / \"data\"\n",
    "DATA_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "ZIP_PATH = DATA_ROOT / \"processed.zip\"\n",
    "\n",
    "if not ZIP_PATH.exists():\n",
    "    print(\"Downloading dataset...\")\n",
    "    r = requests.get(DATA_URL, stream=True)\n",
    "    r.raise_for_status()\n",
    "    with open(ZIP_PATH, \"wb\") as f:\n",
    "        for chunk in r.iter_content(chunk_size=8192):\n",
    "            f.write(chunk)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(\"Dataset already downloaded.\")\n",
    "\n",
    "print(\"Extracting dataset...\")\n",
    "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
    "    z.extractall(DATA_ROOT)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# this is where the per-ticker folders live: processed/<TICKER>/\n",
    "DATA_DIR = DATA_ROOT / \"processed\"\n",
    "print(\"Processed data directory:\", DATA_DIR)\n",
    "\n",
    "# pick a mixed set across sectors and macro.\n",
    "TICKERS = [\"AMAT\", \"AMT\", \"DVN\", \"EA\", \"EXPE\", \"KO\", \"PEP\", \"TXN\", \"USO\", \"XLF\"]\n",
    "\n",
    "# define the required price columns.\n",
    "REQUIRED_PRICE_COLS = [\"date\", \"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\", \"ticker\"]\n",
    "\n",
    "# define the required text columns.\n",
    "REQUIRED_TEXT_COLS  = [\"date\", \"summary\", \"ticker\"]\n",
    "\n",
    "def _find_file(ticker: str, kind: str) -> Path:\n",
    "    # with the new layout we expect: data/processed/<TICKER>/<kind>.csv\n",
    "    p = DATA_DIR / ticker / f\"{kind}.csv\"\n",
    "    if p.exists():\n",
    "        return p\n",
    "    raise FileNotFoundError(f\"Could not find {kind}.csv for {ticker} at {p}\")\n",
    "\n",
    "def _normalize_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [\n",
    "        c.strip()\n",
    "         .lower()\n",
    "         .replace(\"_\", \" \")\n",
    "         .replace(\"-\", \" \")\n",
    "        for c in df.columns\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def _rename_canonical_prices(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _normalize_cols(df)\n",
    "    mapping = {\n",
    "        \"date time\": \"date\",\n",
    "        \"datetime\": \"date\",\n",
    "        \"date\": \"date\",\n",
    "        \"open\": \"open\",\n",
    "        \"high\": \"high\",\n",
    "        \"low\": \"low\",\n",
    "        \"close\": \"close\",\n",
    "        \"adj close\": \"adj_close\",\n",
    "        \"adjusted close\": \"adj_close\",\n",
    "        \"adjclose\": \"adj_close\",\n",
    "        \"adj_close\": \"adj_close\",\n",
    "        \"volume\": \"volume\",\n",
    "        \"record id\": \"record_id\",\n",
    "        \"record_id\": \"record_id\",\n",
    "    }\n",
    "    df = df.rename(columns={c: mapping.get(c, c) for c in df.columns})\n",
    "    return df\n",
    "\n",
    "def _rename_canonical_text(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = _normalize_cols(df)\n",
    "    mapping = {\n",
    "        \"date time\": \"date\",\n",
    "        \"datetime\": \"date\",\n",
    "        \"date\": \"date\",\n",
    "        \"summary\": \"summary\",\n",
    "        \"text\": \"summary\",\n",
    "        \"article\": \"summary\",\n",
    "    }\n",
    "    df = df.rename(columns={c: mapping.get(c, c) for c in df.columns})\n",
    "    return df\n",
    "\n",
    "def load_and_standardize_ticker(ticker: str):\n",
    "    price_path = _find_file(ticker, \"time_series\")\n",
    "    text_path  = _find_file(ticker, \"text\")\n",
    "\n",
    "    prices = pd.read_csv(price_path)\n",
    "    text   = pd.read_csv(text_path)\n",
    "\n",
    "    prices = _rename_canonical_prices(prices)\n",
    "    text   = _rename_canonical_text(text)\n",
    "\n",
    "    prices[\"date\"] = pd.to_datetime(prices[\"date\"], errors=\"coerce\")\n",
    "    text[\"date\"]   = pd.to_datetime(text[\"date\"],   errors=\"coerce\")\n",
    "\n",
    "    prices[\"ticker\"] = ticker\n",
    "    text[\"ticker\"]   = ticker\n",
    "\n",
    "    missing_price = sorted(set(REQUIRED_PRICE_COLS) - set(prices.columns))\n",
    "    missing_text  = sorted(set(REQUIRED_TEXT_COLS)  - set(text.columns))\n",
    "    if missing_price:\n",
    "        raise ValueError(\n",
    "            f\"[{ticker}] Missing required price cols: {missing_price}. \"\n",
    "            f\"Available: {sorted(prices.columns)}\"\n",
    "        )\n",
    "    if missing_text:\n",
    "        raise ValueError(\n",
    "            f\"[{ticker}] Missing required text cols: {missing_text}. \"\n",
    "            f\"Available: {sorted(text.columns)}\"\n",
    "        )\n",
    "\n",
    "    prices = prices[REQUIRED_PRICE_COLS].copy()\n",
    "    text   = text[REQUIRED_TEXT_COLS].copy()\n",
    "\n",
    "    return prices, text, price_path, text_path\n",
    "\n",
    "# collect standardized data by ticker.\n",
    "prices_by_ticker, text_by_ticker, paths = {}, {}, {}\n",
    "\n",
    "for t in TICKERS:\n",
    "    p, tx, p_path, tx_path = load_and_standardize_ticker(t)\n",
    "    prices_by_ticker[t] = p\n",
    "    text_by_ticker[t] = tx\n",
    "    paths[t] = {\"time_series\": str(p_path), \"text\": str(tx_path)}\n",
    "    print(f\"{t}: prices={len(p):,} rows, text={len(tx):,} rows\")\n",
    "\n",
    "display(prices_by_ticker[\"AMAT\"].head())\n",
    "display(text_by_ticker[\"AMAT\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def normalize_day(df: pd.DataFrame, date_col=\"date\") -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # convert date strings into datetime values.\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "    # round timestamps down to midnight.\n",
    "    df[date_col] = df[date_col].dt.floor(\"D\")\n",
    "    # drop timezone info to avoid merges.\n",
    "    try:\n",
    "        df[date_col] = df[date_col].dt.tz_localize(None)\n",
    "    except (TypeError, AttributeError):\n",
    "        pass\n",
    "    return df\n",
    "\n",
    "for t in TICKERS:\n",
    "    # normalize dates and sort by time.\n",
    "    prices_by_ticker[t] = normalize_day(prices_by_ticker[t], \"date\").sort_values(\"date\").reset_index(drop=True)\n",
    "    text_by_ticker[t]   = normalize_day(text_by_ticker[t], \"date\").sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "    # remove duplicate trading days per ticker.\n",
    "    prices_by_ticker[t] = prices_by_ticker[t].drop_duplicates(subset=[\"ticker\", \"date\"])\n",
    "    # remove duplicate summaries on same day.\n",
    "    text_by_ticker[t]   = text_by_ticker[t].drop_duplicates(subset=[\"ticker\", \"date\", \"summary\"])\n",
    "\n",
    "    # print min and max date ranges.\n",
    "    p = prices_by_ticker[t]\n",
    "    x = text_by_ticker[t]\n",
    "    print(\n",
    "        f\"{t}: prices dates [{p['date'].min().date()} \\u2192 {p['date'].max().date()}], \"\n",
    "        f\"text dates [{x['date'].min().date()} \\u2192 {x['date'].max().date()}], \"\n",
    "        f\"price_rows={len(p):,}, text_rows={len(x):,}\"\n",
    "    )\n",
    "\n",
    "display(prices_by_ticker[\"AMAT\"].head())\n",
    "display(text_by_ticker[\"AMAT\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d515b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# define numeric columns used for prices.\n",
    "PRICE_NUMERIC_COLS = [\"open\", \"high\", \"low\", \"close\", \"adj_close\", \"volume\"]\n",
    "\n",
    "def clean_prices_one_ticker(df: pd.DataFrame, ticker: str) -> tuple[pd.DataFrame, dict]:\n",
    "    df = df.copy()\n",
    "\n",
    "    # check required columns exist in dataframe.\n",
    "    needed = [\"date\", \"ticker\"] + PRICE_NUMERIC_COLS\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"[{ticker}] Missing required columns in prices: {missing}. Found: {sorted(df.columns)}\")\n",
    "\n",
    "    # coerce numeric price fields into floats.\n",
    "    for c in PRICE_NUMERIC_COLS:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # drop rows missing adjusted close values.\n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=[\"adj_close\"]).copy()\n",
    "    dropped_adj = before - len(df)\n",
    "\n",
    "    # drop rows missing date or ticker.\n",
    "    before2 = len(df)\n",
    "    df = df.dropna(subset=[\"date\", \"ticker\"]).copy()\n",
    "    dropped_key = before2 - len(df)\n",
    "\n",
    "    # sort rows by ticker and date.\n",
    "    df = df.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    # dedupe and keep the last record.\n",
    "    dup_mask = df.duplicated(subset=[\"ticker\", \"date\"], keep=\"last\")\n",
    "    dup_count = int(dup_mask.sum())\n",
    "    if dup_count > 0:\n",
    "        df = df[~dup_mask].copy()\n",
    "\n",
    "    # count suspicious values for sanity checks.\n",
    "    neg_vol = int((df[\"volume\"] < 0).sum()) if \"volume\" in df.columns else 0\n",
    "    bad_ohlc = int(((df[\"high\"] < df[\"low\"]) | (df[\"close\"] < 0) | (df[\"open\"] < 0)).sum())\n",
    "\n",
    "    report = {\n",
    "        \"ticker\": ticker,\n",
    "        \"rows_before\": before,\n",
    "        \"dropped_missing_adj_close\": int(dropped_adj),\n",
    "        \"dropped_missing_date_or_ticker\": int(dropped_key),\n",
    "        \"deduped_rows\": dup_count,\n",
    "        \"rows_after\": len(df),\n",
    "        \"min_date\": df[\"date\"].min(),\n",
    "        \"max_date\": df[\"date\"].max(),\n",
    "        \"neg_volume_rows\": neg_vol,\n",
    "        \"bad_ohlc_rows\": bad_ohlc,\n",
    "    }\n",
    "    return df, report\n",
    "\n",
    "# run the cleaner for each ticker.\n",
    "clean_reports = []\n",
    "for t in TICKERS:\n",
    "    cleaned, rep = clean_prices_one_ticker(prices_by_ticker[t], t)\n",
    "    prices_by_ticker[t] = cleaned\n",
    "    clean_reports.append(rep)\n",
    "\n",
    "# summarize cleaning effects across tickers.\n",
    "clean_reports_df = pd.DataFrame(clean_reports)\n",
    "display(clean_reports_df)\n",
    "\n",
    "# preview one cleaned ticker dataset.\n",
    "display(prices_by_ticker[\"AMAT\"].head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_text_one_ticker(df: pd.DataFrame, ticker: str) -> tuple[pd.DataFrame, dict]:\n",
    "    df = df.copy()\n",
    "\n",
    "    # check required text columns exist.\n",
    "    needed = [\"date\", \"summary\", \"ticker\"]\n",
    "    missing = [c for c in needed if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"[{ticker}] Missing required columns in text: {missing}. Found: {sorted(df.columns)}\")\n",
    "\n",
    "    before = len(df)\n",
    "\n",
    "    # cast summary to pandas string dtype.\n",
    "    df[\"summary\"] = df[\"summary\"].astype(\"string\")\n",
    "    # drop rows missing summary text.\n",
    "    df = df.dropna(subset=[\"summary\"]).copy()\n",
    "    # drop rows with empty summary text.\n",
    "    df = df[df[\"summary\"].str.strip().ne(\"\")].copy()\n",
    "\n",
    "    dropped_missing_or_empty = before - len(df)\n",
    "\n",
    "    # remove exact duplicate text entries.\n",
    "    before2 = len(df)\n",
    "    df = df.drop_duplicates(subset=[\"ticker\", \"date\", \"summary\"]).copy()\n",
    "    deduped = before2 - len(df)\n",
    "\n",
    "    # sort rows for stable downstream merges.\n",
    "    df = df.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "\n",
    "    report = {\n",
    "        \"ticker\": ticker,\n",
    "        \"rows_before\": before,\n",
    "        \"dropped_missing_or_empty_summary\": int(dropped_missing_or_empty),\n",
    "        \"deduped_exact_rows\": int(deduped),\n",
    "        \"rows_after\": len(df),\n",
    "        \"min_date\": df[\"date\"].min(),\n",
    "        \"max_date\": df[\"date\"].max(),\n",
    "    }\n",
    "    return df, report\n",
    "\n",
    "# run the cleaner for each ticker.\n",
    "text_reports = []\n",
    "for t in TICKERS:\n",
    "    cleaned, rep = clean_text_one_ticker(text_by_ticker[t], t)\n",
    "    text_by_ticker[t] = cleaned\n",
    "    text_reports.append(rep)\n",
    "\n",
    "# summarize text cleaning across tickers.\n",
    "text_reports_df = pd.DataFrame(text_reports)\n",
    "display(text_reports_df)\n",
    "\n",
    "# preview one cleaned text dataset.\n",
    "display(text_by_ticker[\"AMAT\"].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2376b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating trading-day calendar (per ticker)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "trading_calendar = {}\n",
    "calendar_reports = []\n",
    "\n",
    "for t in TICKERS:\n",
    "    p = prices_by_ticker[t].copy()\n",
    "\n",
    "    # for valid trading dates (sorted unique)\n",
    "    cal = pd.Index(p[\"date\"].sort_values().unique(), name=\"date\")\n",
    "    trading_calendar[t] = cal\n",
    "\n",
    "    calendar_reports.append({\n",
    "        \"ticker\": t,\n",
    "        \"n_trading_days\": int(len(cal)),\n",
    "        \"min_date\": cal.min(),\n",
    "        \"max_date\": cal.max(),\n",
    "    })\n",
    "\n",
    "calendar_reports_df = pd.DataFrame(calendar_reports)\n",
    "display(calendar_reports_df)\n",
    "\n",
    "# example to show first/last 5 dates for AMAT\n",
    "t = \"AMAT\"\n",
    "print(f\"{t} first 5 trading days: {trading_calendar[t][:5].tolist()}\")\n",
    "print(f\"{t} last  5 trading days: {trading_calendar[t][-5:].tolist()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00009841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming prev step output, calendar report + spot-check vs price row \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "calendar_reports = []\n",
    "for t in TICKERS:\n",
    "    cal = trading_calendar[t]\n",
    "    calendar_reports.append({\n",
    "        \"ticker\": t,\n",
    "        \"price_rows\": int(len(prices_by_ticker[t])),\n",
    "        \"n_trading_days\": int(len(cal)),\n",
    "        \"min_date\": cal.min(),\n",
    "        \"max_date\": cal.max(),\n",
    "        \"matches_price_rows\": int(len(prices_by_ticker[t])) == int(len(cal)),\n",
    "    })\n",
    "\n",
    "calendar_reports_df = pd.DataFrame(calendar_reports).sort_values(\"ticker\").reset_index(drop=True)\n",
    "display(calendar_reports_df)\n",
    "\n",
    "for t in TICKERS:\n",
    "    print(f\"{t} first 5 trading days: {trading_calendar[t][:5].tolist()}\")\n",
    "    print(f\"{t} last  5 trading days: {trading_calendar[t][-5:].tolist()}\")\n",
    "    print(\"-\" * 60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6094d247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# store aligned text per ticker.\n",
    "aligned_text_by_ticker = {}\n",
    "# store alignment reports per ticker.\n",
    "align_reports = []\n",
    "\n",
    "def align_text_to_calendar(text_df: pd.DataFrame, cal: pd.Index, ticker: str):\n",
    "    text_df = text_df.copy()\n",
    "\n",
    "    # coerce dates to day-level timestamps.\n",
    "    text_df[\"date\"] = pd.to_datetime(text_df[\"date\"], errors=\"coerce\").dt.floor(\"D\")\n",
    "\n",
    "    # convert calendar to sorted datetime array.\n",
    "    trading_days = np.array(pd.to_datetime(cal).sort_values().unique(), dtype=\"datetime64[ns]\")\n",
    "    if len(trading_days) == 0:\n",
    "        raise ValueError(f\"[{ticker}] Trading calendar is empty.\")\n",
    "\n",
    "    # drop rows with invalid dates.\n",
    "    n_before = len(text_df)\n",
    "    text_df = text_df.dropna(subset=[\"date\"]).copy()\n",
    "    n_after_valid = len(text_df)\n",
    "\n",
    "    # extract news dates as datetime64 array.\n",
    "    d = text_df[\"date\"].values.astype(\"datetime64[ns]\")\n",
    "\n",
    "    # find the next trading day index.\n",
    "    idx = np.searchsorted(trading_days, d, side=\"left\").astype(np.int64)\n",
    "\n",
    "    # mark rows that can be mapped.\n",
    "    can_map = idx < len(trading_days)\n",
    "\n",
    "    # create mapped dates with nat defaults.\n",
    "    mapped = np.full(shape=len(idx), fill_value=np.datetime64(\"NaT\", \"ns\"), dtype=\"datetime64[ns]\")\n",
    "    mapped[can_map] = trading_days[idx[can_map]]\n",
    "\n",
    "    # write aligned dates back to dataframe.\n",
    "    text_df[\"aligned_date\"] = pd.to_datetime(mapped)\n",
    "\n",
    "    # drop rows that cannot be aligned.\n",
    "    n_after_map = len(text_df)\n",
    "    text_df = text_df.dropna(subset=[\"aligned_date\"]).copy()\n",
    "\n",
    "    # count how many stayed on same day.\n",
    "    same_day = int(\n",
    "        (text_df[\"aligned_date\"].values.astype(\"datetime64[ns]\") ==\n",
    "         text_df[\"date\"].values.astype(\"datetime64[ns]\")).sum()\n",
    "    )\n",
    "    # count how many shifted forward.\n",
    "    moved_forward = int(len(text_df) - same_day)\n",
    "\n",
    "    dropped_invalid = int(n_before - n_after_valid)\n",
    "    dropped_unmappable = int(n_after_map - len(text_df))\n",
    "\n",
    "    report = {\n",
    "        \"ticker\": ticker,\n",
    "        \"text_rows_before\": int(n_before),\n",
    "        \"dropped_invalid_date\": dropped_invalid,\n",
    "        \"dropped_after_last_trading_day\": dropped_unmappable,\n",
    "        \"text_rows_after\": int(len(text_df)),\n",
    "        \"same_day\": same_day,\n",
    "        \"moved_forward\": moved_forward,\n",
    "        \"calendar_min\": pd.to_datetime(trading_days[0]).date(),\n",
    "        \"calendar_max\": pd.to_datetime(trading_days[-1]).date(),\n",
    "    }\n",
    "    return text_df.reset_index(drop=True), report\n",
    "\n",
    "# align each ticker text to trading dates.\n",
    "for t in TICKERS:\n",
    "    aligned, rep = align_text_to_calendar(text_by_ticker[t], trading_calendar[t], t)\n",
    "    aligned_text_by_ticker[t] = aligned\n",
    "    align_reports.append(rep)\n",
    "\n",
    "# show alignment stats across tickers.\n",
    "align_reports_df = pd.DataFrame(align_reports).sort_values(\"ticker\").reset_index(drop=True)\n",
    "display(align_reports_df)\n",
    "\n",
    "# show examples shifted to next trading day.\n",
    "for t in TICKERS:\n",
    "    moved_examples = aligned_text_by_ticker[t].query(\"aligned_date != date\")[[\"date\", \"aligned_date\", \"ticker\", \"summary\"]].head(5)\n",
    "    print(f\"\\n{t}: moved-forward examples (if any)\")\n",
    "    display(moved_examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f404d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip -q install -U transformers torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f864af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_articles = pd.concat(\n",
    "    [aligned_text_by_ticker[t].assign(ticker=t) for t in TICKERS],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# We'll use aligned_date as the \"effective\" date for modeling\n",
    "all_articles = all_articles.rename(columns={\"aligned_date\": \"date\"})\n",
    "all_articles = all_articles[[\"date\", \"ticker\", \"summary\"]].copy()\n",
    "all_articles.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef60c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"  # per HF guidance :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "MODEL_ID = \"ProsusAI/finbert\"  # :contentReference[oaicite:5]{index=5}\n",
    "\n",
    "# Pick device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID).to(device)\n",
    "model.eval()\n",
    "\n",
    "id2label = model.config.id2label\n",
    "label2id = {v.lower(): k for k, v in id2label.items()}  # e.g., positive/negative/neutral\n",
    "\n",
    "def finbert_score(texts, batch_size=32, max_length=256):\n",
    "    all_pos, all_neg, all_neu = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "            logits = model(**enc).logits\n",
    "            probs = torch.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "            # Map label indices robustly\n",
    "            # common labels: \"positive\", \"negative\", \"neutral\"\n",
    "            pos_idx = label2id.get(\"positive\")\n",
    "            neg_idx = label2id.get(\"negative\")\n",
    "            neu_idx = label2id.get(\"neutral\")\n",
    "\n",
    "            all_pos.append(probs[:, pos_idx] if pos_idx is not None else np.full(len(batch), np.nan))\n",
    "            all_neg.append(probs[:, neg_idx] if neg_idx is not None else np.full(len(batch), np.nan))\n",
    "            all_neu.append(probs[:, neu_idx] if neu_idx is not None else np.full(len(batch), np.nan))\n",
    "\n",
    "    pos = np.concatenate(all_pos)\n",
    "    neg = np.concatenate(all_neg)\n",
    "    neu = np.concatenate(all_neu)\n",
    "    score = pos - neg\n",
    "    return pos, neg, neu, score\n",
    "\n",
    "texts = all_articles[\"summary\"].astype(str).tolist()\n",
    "pos, neg, neu, score = finbert_score(texts, batch_size=32, max_length=256)\n",
    "\n",
    "all_articles[\"sent_pos\"] = pos\n",
    "all_articles[\"sent_neg\"] = neg\n",
    "all_articles[\"sent_neu\"] = neu\n",
    "all_articles[\"sent_score\"] = score\n",
    "\n",
    "all_articles.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777a8fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# drop duplicate date columns to avoid pandas weirdness.\n",
    "all_articles = all_articles.loc[:, ~all_articles.columns.duplicated()].copy()\n",
    "\n",
    "# check probabilities sum close to one.\n",
    "prob_sum = all_articles[[\"sent_pos\", \"sent_neg\", \"sent_neu\"]].sum(axis=1)\n",
    "print(\"prob_sum stats:\\n\", prob_sum.describe())\n",
    "print(\"rows far from 1.0:\", int((prob_sum - 1.0).abs().gt(1e-3).sum()))\n",
    "\n",
    "# add a confidence feature (useful for weighting).\n",
    "all_articles[\"sent_conf\"] = 1.0 - all_articles[\"sent_neu\"]\n",
    "\n",
    "# summarize sentiment by ticker.\n",
    "ticker_summary = (\n",
    "    all_articles.groupby(\"ticker\")\n",
    "    .agg(\n",
    "        n=(\"sent_score\", \"size\"),\n",
    "        mean_score=(\"sent_score\", \"mean\"),\n",
    "        median_score=(\"sent_score\", \"median\"),\n",
    "        mean_neu=(\"sent_neu\", \"mean\"),\n",
    "        mean_conf=(\"sent_conf\", \"mean\"),\n",
    "        frac_strong=(\"sent_score\", lambda s: float((s.abs() >= 0.5).mean())),\n",
    "        frac_pos=(\"sent_score\", lambda s: float((s > 0.1).mean())),\n",
    "        frac_neg=(\"sent_score\", lambda s: float((s < -0.1).mean())),\n",
    "    )\n",
    "    .sort_values(\"mean_score\", ascending=False)\n",
    ")\n",
    "display(ticker_summary)\n",
    "\n",
    "# plot 1: boxplot of sentiment score per ticker.\n",
    "tickers = ticker_summary.index.tolist()\n",
    "data = [all_articles.loc[all_articles[\"ticker\"] == t, \"sent_score\"].to_numpy() for t in tickers]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.boxplot(data, labels=tickers, showfliers=False)\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.title(\"finbert sentiment score distribution by ticker\")\n",
    "plt.ylabel(\"sent_score (pos - neg)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot 2: mean sentiment and mean neutrality per ticker.\n",
    "plt.figure(figsize=(12, 4))\n",
    "x = np.arange(len(tickers))\n",
    "plt.bar(x - 0.2, ticker_summary[\"mean_score\"].to_numpy(), width=0.4, label=\"mean score\")\n",
    "plt.bar(x + 0.2, ticker_summary[\"mean_neu\"].to_numpy(),  width=0.4, label=\"mean neutral prob\")\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.xticks(x, tickers)\n",
    "plt.title(\"average sentiment vs average neutrality by ticker\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142d55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sample = all_articles[\"summary\"].astype(str).head(200).tolist()\n",
    "\n",
    "t0 = time.time()\n",
    "_ = finbert_score(sample, batch_size=32, max_length=256)\n",
    "t1 = time.time()\n",
    "\n",
    "print(\"Seconds for 200 summaries:\", round(t1 - t0, 3))\n",
    "print(\"Summaries/sec:\", round(200 / (t1 - t0), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c04d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# print table shape and basic previews.\n",
    "print(\"shape:\", all_articles.shape)\n",
    "display(all_articles.head(10))\n",
    "display(all_articles.tail(10))\n",
    "# list columns and data types.\n",
    "print(\"\\ncolumns:\", list(all_articles.columns))\n",
    "print(\"\\ndtypes:\\n\", all_articles.dtypes)\n",
    "\n",
    "# check if any column names repeat.\n",
    "dupe_cols = all_articles.columns[all_articles.columns.duplicated()].tolist()\n",
    "print(\"\\nduplicate column names:\", dupe_cols)\n",
    "\n",
    "# drop repeated columns and keep first.\n",
    "if dupe_cols:\n",
    "    all_articles = all_articles.loc[:, ~all_articles.columns.duplicated()].copy()\n",
    "    print(\"\\nDropped duplicate columns. New columns:\", list(all_articles.columns))\n",
    "\n",
    "# convert date to day-level datetime.\n",
    "all_articles[\"date\"] = pd.to_datetime(all_articles[\"date\"], errors=\"coerce\").dt.floor(\"D\")\n",
    "print(\"\\nDate range:\", all_articles[\"date\"].min(), \"→\", all_articles[\"date\"].max())\n",
    "\n",
    "# count missing values in key fields.\n",
    "key_cols = [\"date\", \"ticker\", \"summary\", \"sent_pos\", \"sent_neg\", \"sent_neu\", \"sent_score\"]\n",
    "missing = all_articles[key_cols].isna().sum().sort_values(ascending=False)\n",
    "print(\"\\nMissing values (key cols):\")\n",
    "display(missing)\n",
    "\n",
    "# summarize per-ticker coverage and average length.\n",
    "ticker_stats = (\n",
    "    all_articles.groupby(\"ticker\")\n",
    "    .agg(\n",
    "        articles=(\"summary\", \"count\"),\n",
    "        min_date=(\"date\", \"min\"),\n",
    "        max_date=(\"date\", \"max\"),\n",
    "        avg_len=(\"summary\", lambda s: s.astype(str).str.len().mean())\n",
    "    )\n",
    "    .sort_values(\"articles\", ascending=False)\n",
    ")\n",
    "display(ticker_stats)\n",
    "\n",
    "# describe sentiment columns to sanity check.\n",
    "display(all_articles[[\"sent_pos\",\"sent_neg\",\"sent_neu\",\"sent_score\"]].describe())\n",
    "\n",
    "# count articles per ticker per day.\n",
    "per_day = (\n",
    "    all_articles.groupby([\"ticker\",\"date\"])\n",
    "    .size()\n",
    "    .rename(\"articles_that_day\")\n",
    "    .reset_index()\n",
    "    .sort_values(\"articles_that_day\", ascending=False)\n",
    ")\n",
    "print(\"\\nTop 20 (ticker, day) by number of articles:\")\n",
    "display(per_day.head(20))\n",
    "\n",
    "# summarize daily article count distribution.\n",
    "print(\"\\nDistribution of articles per day (all tickers):\")\n",
    "display(per_day[\"articles_that_day\"].describe())\n",
    "\n",
    "# show one extreme day for inspection.\n",
    "if len(per_day) > 0:\n",
    "    ex = per_day.iloc[0]\n",
    "    tkr, day = ex[\"ticker\"], ex[\"date\"]\n",
    "    print(f\"\\nExample heavy-news day: ticker={tkr}, date={day.date()}, count={int(ex['articles_that_day'])}\")\n",
    "    display(\n",
    "        all_articles[(all_articles[\"ticker\"] == tkr) & (all_articles[\"date\"] == day)]\n",
    "        [[\"date\",\"ticker\",\"sent_score\",\"summary\"]]\n",
    "        .head(10)\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759d169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# align article dates to trading days.\n",
    "def add_aligned_date(all_articles: pd.DataFrame, trading_calendar: dict) -> pd.DataFrame:\n",
    "    df = all_articles.copy()\n",
    "\n",
    "    # coerce date to day-level datetime.\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.floor(\"D\")\n",
    "\n",
    "    # create empty aligned date column.\n",
    "    df[\"aligned_date\"] = pd.NaT\n",
    "\n",
    "    # align each ticker using its calendar.\n",
    "    for t, g in df.groupby(\"ticker\", sort=False):\n",
    "        if t not in trading_calendar:\n",
    "            continue\n",
    "\n",
    "        # build sorted trading day array.\n",
    "        trading_days = np.array(\n",
    "            pd.to_datetime(trading_calendar[t]).sort_values().unique(),\n",
    "            dtype=\"datetime64[ns]\"\n",
    "        )\n",
    "        if len(trading_days) == 0:\n",
    "            continue\n",
    "\n",
    "        d = g[\"date\"].values.astype(\"datetime64[ns]\")\n",
    "        idx = np.searchsorted(trading_days, d, side=\"left\").astype(np.int64)\n",
    "        can_map = idx < len(trading_days)\n",
    "\n",
    "        mapped = np.full(len(idx), np.datetime64(\"NaT\", \"ns\"), dtype=\"datetime64[ns]\")\n",
    "        mapped[can_map] = trading_days[idx[can_map]]\n",
    "\n",
    "        df.loc[g.index, \"aligned_date\"] = pd.to_datetime(mapped)\n",
    "\n",
    "    # drop rows that cannot be aligned.\n",
    "    df = df.dropna(subset=[\"aligned_date\"]).copy()\n",
    "\n",
    "    # compute how far dates were shifted.\n",
    "    df[\"shift_days\"] = (df[\"aligned_date\"] - df[\"date\"]).dt.days.astype(\"int32\")\n",
    "    df[\"moved_forward\"] = df[\"shift_days\"] > 0\n",
    "    return df\n",
    "\n",
    "# build daily panel by merging prices and news.\n",
    "def build_panel(prices_by_ticker: dict, articles_aligned: pd.DataFrame) -> pd.DataFrame:\n",
    "    panels = []\n",
    "\n",
    "    # aggregate in case multiple news map same day.\n",
    "    daily_news = (\n",
    "        articles_aligned.groupby([\"ticker\", \"aligned_date\"])\n",
    "        .agg(\n",
    "            news_count=(\"summary\", \"size\"),\n",
    "            sent_score_obs=(\"sent_score\", \"mean\"),\n",
    "            sent_conf_obs=(\"sent_conf\", \"mean\"),\n",
    "            sent_abs_obs=(\"sent_score\", lambda s: float(np.mean(np.abs(s)))),\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # merge daily news onto each ticker prices.\n",
    "    for t, p in prices_by_ticker.items():\n",
    "        p = p.copy()\n",
    "        p[\"date\"] = pd.to_datetime(p[\"date\"], errors=\"coerce\").dt.floor(\"D\")\n",
    "\n",
    "        m = p.merge(\n",
    "            daily_news[daily_news[\"ticker\"] == t],\n",
    "            left_on=[\"ticker\", \"date\"],\n",
    "            right_on=[\"ticker\", \"aligned_date\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "\n",
    "        # define news presence on trading days.\n",
    "        m[\"news_count\"] = m[\"news_count\"].fillna(0).astype(\"int32\")\n",
    "        m[\"has_news\"] = (m[\"news_count\"] > 0).astype(\"int8\")\n",
    "\n",
    "        # keep a clean sort order.\n",
    "        m = m.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "        panels.append(m)\n",
    "\n",
    "    return pd.concat(panels, ignore_index=True)\n",
    "\n",
    "# compute days since last news event.\n",
    "def add_days_since_news(panel: pd.DataFrame) -> pd.DataFrame:\n",
    "    panel = panel.copy()\n",
    "\n",
    "    # create event groups that increment on news.\n",
    "    panel[\"event_group\"] = panel.groupby(\"ticker\")[\"has_news\"].cumsum()\n",
    "\n",
    "    # count days within each event group.\n",
    "    panel[\"days_since_news\"] = panel.groupby([\"ticker\", \"event_group\"]).cumcount().astype(\"float32\")\n",
    "\n",
    "    # set pre-first-news days to nan.\n",
    "    pre_first = (panel[\"event_group\"] == 0) & (panel[\"has_news\"] == 0)\n",
    "    panel.loc[pre_first, \"days_since_news\"] = np.nan\n",
    "\n",
    "    return panel.drop(columns=[\"event_group\"])\n",
    "\n",
    "# apply exponential decay on no-news days.\n",
    "def add_exponential_decay(panel: pd.DataFrame, lam: float = 0.03, baseline: float = 0.0) -> pd.DataFrame:\n",
    "    panel = panel.copy()\n",
    "    decay = float(np.exp(-lam))\n",
    "\n",
    "    # compute per-ticker decayed sentiment feature.\n",
    "    out = []\n",
    "    for t, g in panel.groupby(\"ticker\", sort=False):\n",
    "        g = g.sort_values(\"date\").copy()\n",
    "        s_obs = g[\"sent_score_obs\"].to_numpy(dtype=float)\n",
    "        has = g[\"has_news\"].to_numpy(dtype=bool)\n",
    "\n",
    "        s_feat = np.empty(len(g), dtype=float)\n",
    "        prev = baseline\n",
    "        for i in range(len(g)):\n",
    "            if has[i] and not np.isnan(s_obs[i]):\n",
    "                prev = float(s_obs[i])\n",
    "            else:\n",
    "                prev = baseline + (prev - baseline) * decay\n",
    "            s_feat[i] = prev\n",
    "\n",
    "        g[\"sent_score_decay\"] = s_feat.astype(\"float32\")\n",
    "        out.append(g)\n",
    "\n",
    "    return pd.concat(out, ignore_index=True)\n",
    "\n",
    "# build aligned articles with shift stats.\n",
    "articles_aligned = add_aligned_date(all_articles, trading_calendar)\n",
    "\n",
    "# report: shifted vs same-day, per ticker.\n",
    "shift_report = (\n",
    "    articles_aligned.groupby(\"ticker\")\n",
    "    .agg(\n",
    "        n_articles=(\"summary\", \"size\"),\n",
    "        same_day=(\"moved_forward\", lambda x: int((~x).sum())),\n",
    "        moved_forward=(\"moved_forward\", \"sum\"),\n",
    "        mean_shift_days=(\"shift_days\", \"mean\"),\n",
    "        max_shift_days=(\"shift_days\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"ticker\")\n",
    ")\n",
    "display(shift_report)\n",
    "\n",
    "# build full trading-day panel and add sparse-news features.\n",
    "panel = build_panel(prices_by_ticker, articles_aligned)\n",
    "panel = add_days_since_news(panel)\n",
    "panel = add_exponential_decay(panel, lam=0.03, baseline=0.0)\n",
    "\n",
    "# report: news coverage and gap sizes.\n",
    "coverage_report = (\n",
    "    panel.groupby(\"ticker\")\n",
    "    .agg(\n",
    "        n_trading_days=(\"date\", \"size\"),\n",
    "        n_news_days=(\"has_news\", \"sum\"),\n",
    "        pct_news_days=(\"has_news\", \"mean\"),\n",
    "        max_days_since_news=(\"days_since_news\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"pct_news_days\")\n",
    ")\n",
    "display(coverage_report)\n",
    "\n",
    "# quick peek at the final panel.\n",
    "display(panel[[\"ticker\",\"date\",\"has_news\",\"news_count\",\"sent_score_obs\",\"sent_score_decay\",\"days_since_news\"]].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e660f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# where are the largest gaps?\n",
    "gap_rows = (panel.dropna(subset=[\"days_since_news\"])\n",
    "            .sort_values(\"days_since_news\", ascending=False)\n",
    "            .loc[:, [\"ticker\",\"date\",\"days_since_news\",\"has_news\",\"sent_score_decay\"]]\n",
    "            .head(30))\n",
    "display(gap_rows)\n",
    "\n",
    "# gap distribution per ticker\n",
    "gap_stats = (panel.dropna(subset=[\"days_since_news\"])\n",
    "             .groupby(\"ticker\")[\"days_since_news\"]\n",
    "             .agg([\"count\",\"mean\",\"median\",\"max\"])\n",
    "             .sort_values(\"max\", ascending=False))\n",
    "display(gap_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f2b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "t = \"AMT\"\n",
    "g = panel[panel[\"ticker\"] == t].sort_values(\"date\").reset_index(drop=True)\n",
    "\n",
    "# find the row with the max gap\n",
    "imax = g[\"days_since_news\"].fillna(-1).idxmax()\n",
    "row = g.loc[imax, [\"date\",\"days_since_news\",\"sent_score_decay\",\"has_news\"]]\n",
    "print(\"max gap row:\\n\", row)\n",
    "\n",
    "# last news day before it\n",
    "prev_news = g.loc[:imax][g.loc[:imax, \"has_news\"] == 1].tail(1)\n",
    "# next news day after it\n",
    "next_news = g.loc[imax:][g.loc[imax:, \"has_news\"] == 1].head(1)\n",
    "\n",
    "print(\"\\nprevious news day:\")\n",
    "display(prev_news[[\"date\",\"has_news\",\"news_count\",\"sent_score_obs\"]])\n",
    "\n",
    "print(\"\\nnext news day:\")\n",
    "display(next_news[[\"date\",\"has_news\",\"news_count\",\"sent_score_obs\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0b760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# compute rmse using a sqrt wrapper.\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# create next-day targets for modelling.\n",
    "panel = panel.sort_values([\"ticker\", \"date\"]).reset_index(drop=True)\n",
    "panel[\"log_adj_close\"] = np.log(panel[\"adj_close\"].astype(float))\n",
    "panel[\"target_nextday_logret\"] = panel.groupby(\"ticker\")[\"log_adj_close\"].diff(-1) * -1\n",
    "panel[\"target_up\"] = (panel[\"target_nextday_logret\"] > 0).astype(\"int8\")\n",
    "panel = panel.dropna(subset=[\"target_nextday_logret\"]).copy()\n",
    "\n",
    "# define a volatility style target.\n",
    "panel[\"target_abs_logret\"] = panel[\"target_nextday_logret\"].abs()\n",
    "\n",
    "# create baseline price and volume features.\n",
    "panel[\"ret1\"] = panel.groupby(\"ticker\")[\"adj_close\"].pct_change()\n",
    "panel[\"vol_z\"] = panel.groupby(\"ticker\")[\"volume\"].transform(lambda s: (s - s.mean()) / (s.std() + 1e-9))\n",
    "\n",
    "# do ticker-wise z score for sentiment.\n",
    "g = panel.groupby(\"ticker\")[\"sent_score_decay\"]\n",
    "panel[\"sent_decay_z\"] = (panel[\"sent_score_decay\"] - g.transform(\"mean\")) / (g.transform(\"std\") + 1e-9)\n",
    "\n",
    "# build a rolling baseline and surprise signal.\n",
    "panel[\"sent_z_roll20\"] = panel.groupby(\"ticker\")[\"sent_decay_z\"].transform(\n",
    "    lambda s: s.rolling(20, min_periods=5).mean()\n",
    ")\n",
    "panel[\"sent_surprise\"] = panel[\"sent_decay_z\"] - panel[\"sent_z_roll20\"]\n",
    "\n",
    "# add simple return lags for momentum context.\n",
    "for L in [1, 2, 3, 5, 10]:\n",
    "    panel[f\"ret_lag{L}\"] = panel.groupby(\"ticker\")[\"ret1\"].shift(L)\n",
    "\n",
    "# add rolling mean and rolling std features.\n",
    "panel[\"ret_roll5\"] = panel.groupby(\"ticker\")[\"ret1\"].transform(lambda s: s.rolling(5, min_periods=5).mean())\n",
    "panel[\"vol_roll5\"] = panel.groupby(\"ticker\")[\"ret1\"].transform(lambda s: s.rolling(5, min_periods=5).std())\n",
    "\n",
    "# optionally cap very long no-news gaps.\n",
    "RESET_AFTER = 60\n",
    "panel[\"sent_score_final\"] = panel[\"sent_score_decay\"].where(panel[\"days_since_news\"] <= RESET_AFTER, 0.0)\n",
    "panel[\"sent_final_z\"] = panel.groupby(\"ticker\")[\"sent_score_final\"].transform(\n",
    "    lambda s: (s - s.mean()) / (s.std() + 1e-9)\n",
    ")\n",
    "\n",
    "# drop rows created by lagging and rolling.\n",
    "panel_ml = panel.dropna().copy()\n",
    "\n",
    "# split by time to avoid leakage.\n",
    "dates = panel_ml[\"date\"].sort_values().unique()\n",
    "cut1 = dates[int(0.70 * len(dates))]\n",
    "cut2 = dates[int(0.85 * len(dates))]\n",
    "\n",
    "train = panel_ml[panel_ml[\"date\"] <= cut1].copy()\n",
    "val   = panel_ml[(panel_ml[\"date\"] > cut1) & (panel_ml[\"date\"] <= cut2)].copy()\n",
    "test  = panel_ml[panel_ml[\"date\"] > cut2].copy()\n",
    "\n",
    "print(\n",
    "    \"splits:\",\n",
    "    train[\"date\"].min().date(), \"→\", train[\"date\"].max().date(),\n",
    "    \"|\", val[\"date\"].min().date(), \"→\", val[\"date\"].max().date(),\n",
    "    \"|\", test[\"date\"].min().date(), \"→\", test[\"date\"].max().date(),\n",
    ")\n",
    "\n",
    "# compare feature sets for volatility prediction.\n",
    "feature_sets = {\n",
    "    \"price_only\": [\n",
    "        \"ret1\", \"vol_z\", \"ret_roll5\", \"vol_roll5\",\n",
    "        \"ret_lag1\", \"ret_lag2\", \"ret_lag3\", \"ret_lag5\", \"ret_lag10\",\n",
    "    ],\n",
    "    \"plus_decay_raw\": [\n",
    "        \"ret1\", \"vol_z\", \"ret_roll5\", \"vol_roll5\",\n",
    "        \"ret_lag1\", \"ret_lag2\", \"ret_lag3\", \"ret_lag5\", \"ret_lag10\",\n",
    "        \"sent_score_decay\",\n",
    "    ],\n",
    "    \"plus_decay_z\": [\n",
    "        \"ret1\", \"vol_z\", \"ret_roll5\", \"vol_roll5\",\n",
    "        \"ret_lag1\", \"ret_lag2\", \"ret_lag3\", \"ret_lag5\", \"ret_lag10\",\n",
    "        \"sent_decay_z\",\n",
    "    ],\n",
    "    \"plus_surprise\": [\n",
    "        \"ret1\", \"vol_z\", \"ret_roll5\", \"vol_roll5\",\n",
    "        \"ret_lag1\", \"ret_lag2\", \"ret_lag3\", \"ret_lag5\", \"ret_lag10\",\n",
    "        \"sent_decay_z\", \"sent_surprise\",\n",
    "    ],\n",
    "    \"plus_missingness\": [\n",
    "        \"ret1\", \"vol_z\", \"ret_roll5\", \"vol_roll5\",\n",
    "        \"ret_lag1\", \"ret_lag2\", \"ret_lag3\", \"ret_lag5\", \"ret_lag10\",\n",
    "        \"sent_decay_z\", \"sent_surprise\",\n",
    "        \"has_news\", \"news_count\", \"days_since_news\",\n",
    "    ],\n",
    "    \"plus_reset_rule\": [\n",
    "        \"ret1\", \"vol_z\", \"ret_roll5\", \"vol_roll5\",\n",
    "        \"ret_lag1\", \"ret_lag2\", \"ret_lag3\", \"ret_lag5\", \"ret_lag10\",\n",
    "        \"sent_final_z\", \"sent_surprise\",\n",
    "        \"has_news\", \"news_count\", \"days_since_news\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, feats in feature_sets.items():\n",
    "    Xtr, ytr = train[feats], train[\"target_abs_logret\"]\n",
    "    Xva, yva = val[feats], val[\"target_abs_logret\"]\n",
    "    Xte, yte = test[feats], test[\"target_abs_logret\"]\n",
    "\n",
    "    # train a simple ridge regression baseline.\n",
    "    reg = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"ridge\", Ridge(alpha=1.0, random_state=0)),\n",
    "    ])\n",
    "    reg.fit(Xtr, ytr)\n",
    "\n",
    "    p_va = reg.predict(Xva)\n",
    "    p_te = reg.predict(Xte)\n",
    "\n",
    "    results.append({\n",
    "        \"features\": name,\n",
    "        \"val_mae\": mean_absolute_error(yva, p_va),\n",
    "        \"val_rmse\": rmse(yva, p_va),\n",
    "        \"val_r2\": r2_score(yva, p_va),\n",
    "        \"test_mae\": mean_absolute_error(yte, p_te),\n",
    "        \"test_rmse\": rmse(yte, p_te),\n",
    "        \"test_r2\": r2_score(yte, p_te),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values(\"test_rmse\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a25cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step: xgboost training + metrics (unmodified core logic)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import shap\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# silence tokenizers fork parallelism warning.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# compute rmse using a sqrt wrapper.\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# pick the features you want to explain.\n",
    "best_feats = feature_sets[\"plus_missingness\"]\n",
    "Xtr = train[best_feats]\n",
    "ytr = train[\"target_abs_logret\"].astype(float).to_numpy()\n",
    "Xte = test[best_feats]\n",
    "yte = test[\"target_abs_logret\"].astype(float).to_numpy()\n",
    "\n",
    "# set a scalar base score to avoid shap parsing issues.\n",
    "base = float(np.mean(ytr))\n",
    "\n",
    "# train xgboost regressor for volatility prediction.\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.03,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    base_score=base,\n",
    "    random_state=0,\n",
    ")\n",
    "xgb_model.fit(Xtr, ytr)\n",
    "\n",
    "# print test metrics for the fitted model.\n",
    "pred = xgb_model.predict(Xte)\n",
    "print(\"xgb test mae:\", mean_absolute_error(yte, pred))\n",
    "print(\"xgb test rmse:\", rmse(yte, pred))\n",
    "print(\"xgb test r2:\", r2_score(yte, pred))\n",
    "\n",
    "# step: xgboost shap explainability (unmodified)\n",
    "# try tree explainer first and fall back safely.\n",
    "try:\n",
    "    explainer = shap.TreeExplainer(xgb_model)\n",
    "    shap_values = explainer.shap_values(Xte)\n",
    "    shap.summary_plot(shap_values, Xte)\n",
    "    shap.summary_plot(shap_values, Xte, plot_type=\"bar\")\n",
    "except Exception as e:\n",
    "    print(\"treeexplainer failed, using permutation explainer:\", repr(e))\n",
    "\n",
    "    # use small background for speed and stability.\n",
    "    bg = Xtr.sample(n=min(200, len(Xtr)), random_state=0)\n",
    "    ex = Xte.sample(n=min(500, len(Xte)), random_state=0)\n",
    "\n",
    "    # explain predictions with a model-agnostic method.\n",
    "    explainer = shap.Explainer(xgb_model.predict, bg, algorithm=\"permutation\")\n",
    "    shap_exp = explainer(ex)\n",
    "\n",
    "    shap.summary_plot(shap_exp.values, ex)\n",
    "    shap.summary_plot(shap_exp.values, ex, plot_type=\"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step: lstm + transformer imports and reproducibility\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# reuse rmse name (overwrites previous definition, which is fine)\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "# step: sequence building helpers (panel -> (n, t, f))\n",
    "\n",
    "def _infer_time_col(df):\n",
    "    if \"date\" in df.columns:\n",
    "        return \"date\"\n",
    "    if hasattr(df.index, \"dtype\") and \"datetime\" in str(df.index.dtype).lower():\n",
    "        return None  # use index\n",
    "    return None\n",
    "\n",
    "def _ensure_sort(df, time_col):\n",
    "    if time_col is None:\n",
    "        return df.sort_index()\n",
    "    return df.sort_values(time_col)\n",
    "\n",
    "def _ensure_group_col(df):\n",
    "    if \"ticker\" in df.columns:\n",
    "        return \"ticker\"\n",
    "    tmp = df.copy()\n",
    "    tmp[\"ticker\"] = \"ALL\"\n",
    "    return \"ticker\", tmp\n",
    "\n",
    "def make_sequences(df, feature_cols, target_col, seq_len=20):\n",
    "    time_col = _infer_time_col(df)\n",
    "    group_col, df2 = _ensure_group_col(df) if \"ticker\" not in df.columns else (\"ticker\", df)\n",
    "\n",
    "    X_list, y_list = [], []\n",
    "    for _, g in df2.groupby(group_col):\n",
    "        g = _ensure_sort(g, time_col)\n",
    "        X = g[feature_cols].to_numpy(dtype=np.float32)\n",
    "        y = g[target_col].to_numpy(dtype=np.float32)\n",
    "\n",
    "        if len(g) < seq_len:\n",
    "            continue\n",
    "\n",
    "        for i in range(seq_len - 1, len(g)):\n",
    "            X_list.append(X[i - seq_len + 1 : i + 1])\n",
    "            y_list.append(y[i])\n",
    "\n",
    "    if not X_list:\n",
    "        raise ValueError(\"No sequences were created. Check seq_len and per-ticker data length.\")\n",
    "\n",
    "    return np.stack(X_list, axis=0), np.array(y_list, dtype=np.float32)\n",
    "\n",
    "def split_val_from_train(train_df, val_frac=0.1):\n",
    "    time_col = _infer_time_col(train_df)\n",
    "    group_col, df2 = _ensure_group_col(train_df) if \"ticker\" not in train_df.columns else (\"ticker\", train_df)\n",
    "\n",
    "    train_parts, val_parts = [], []\n",
    "    for _, g in df2.groupby(group_col):\n",
    "        g = _ensure_sort(g, time_col)\n",
    "        n = len(g)\n",
    "        k = max(1, int(round(n * val_frac)))\n",
    "        val_parts.append(g.iloc[-k:])\n",
    "        train_parts.append(g.iloc[:-k] if n > k else g.iloc[:0])\n",
    "\n",
    "    tr = pd.concat(train_parts, axis=0)\n",
    "    va = pd.concat(val_parts, axis=0)\n",
    "    return tr, va\n",
    "\n",
    "\n",
    "# step: torch dataset and models (lstm + transformer)\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)  # (N, T, F)\n",
    "        self.y = torch.from_numpy(y).unsqueeze(-1)  # (N, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden=128, layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_features,\n",
    "            hidden_size=hidden,\n",
    "            num_layers=layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if layers > 1 else 0.0,\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(hidden),\n",
    "            nn.Linear(hidden, hidden // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        last = out[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        T = x.size(1)\n",
    "        return x + self.pe[:, :T, :]\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, n_features, d_model=128, nhead=4, num_layers=2, dim_ff=256, dropout=0.1, max_len=512):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Linear(n_features, d_model)\n",
    "        self.pos = SinusoidalPositionalEncoding(d_model, max_len=max_len)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation=\"gelu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.enc = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.inp(x)\n",
    "        z = self.pos(z)\n",
    "        z = self.enc(z)\n",
    "        last = z[:, -1, :]\n",
    "        return self.head(last)\n",
    "\n",
    "\n",
    "# step: training and prediction loops\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_torch(model, loader, device):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    for xb, yb in loader:\n",
    "        xb = xb.to(device)\n",
    "        yhat = model(xb).detach().cpu().numpy().reshape(-1)\n",
    "        preds.append(yhat)\n",
    "        ys.append(yb.numpy().reshape(-1))\n",
    "    return np.concatenate(ys), np.concatenate(preds)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=30, lr=1e-3, weight_decay=1e-4, patience=5):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    best_state = None\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "\n",
    "    for _ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            yhat = model(xb)\n",
    "            loss = loss_fn(yhat, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "\n",
    "        yv, pv = predict_torch(model, val_loader, device)\n",
    "        val_rmse = rmse(yv, pv)\n",
    "\n",
    "        if val_rmse < best_val - 1e-6:\n",
    "            best_val = val_rmse\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "    return model\n",
    "\n",
    "\n",
    "# step: build sequences, scale features, and create dataloaders\n",
    "\n",
    "SEQ_LEN = 20\n",
    "BATCH = 512\n",
    "EPOCHS = 30\n",
    "target_col = \"target_abs_logret\"\n",
    "\n",
    "if \"val\" in globals():\n",
    "    val_df = val\n",
    "    train_df = train\n",
    "else:\n",
    "    train_df, val_df = split_val_from_train(train, val_frac=0.1)\n",
    "\n",
    "Xtr_seq, ytr_seq = make_sequences(train_df, best_feats, target_col, seq_len=SEQ_LEN)\n",
    "Xva_seq, yva_seq = make_sequences(val_df,   best_feats, target_col, seq_len=SEQ_LEN)\n",
    "Xte_seq, yte_seq = make_sequences(test,     best_feats, target_col, seq_len=SEQ_LEN)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Ntr, T, F = Xtr_seq.shape\n",
    "scaler.fit(Xtr_seq.reshape(Ntr * T, F))\n",
    "\n",
    "def apply_scaler(X):\n",
    "    N, T, F = X.shape\n",
    "    return scaler.transform(X.reshape(N * T, F)).reshape(N, T, F).astype(np.float32)\n",
    "\n",
    "Xtr_seq = apply_scaler(Xtr_seq)\n",
    "Xva_seq = apply_scaler(Xva_seq)\n",
    "Xte_seq = apply_scaler(Xte_seq)\n",
    "\n",
    "train_loader = DataLoader(SeqDataset(Xtr_seq, ytr_seq), batch_size=BATCH, shuffle=True, drop_last=False)\n",
    "val_loader   = DataLoader(SeqDataset(Xva_seq, yva_seq), batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "test_loader  = DataLoader(SeqDataset(Xte_seq, yte_seq), batch_size=BATCH, shuffle=False, drop_last=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# step: train and evaluate lstm model\n",
    "\n",
    "lstm = LSTMRegressor(n_features=F, hidden=128, layers=2, dropout=0.2)\n",
    "lstm = train_model(lstm, train_loader, val_loader, device, epochs=EPOCHS, lr=1e-3, weight_decay=1e-4, patience=5)\n",
    "y_true_lstm, y_pred_lstm = predict_torch(lstm, test_loader, device)\n",
    "print(\"\\nLSTM test mae:\", mean_absolute_error(y_true_lstm, y_pred_lstm))\n",
    "print(\"LSTM test rmse:\", rmse(y_true_lstm, y_pred_lstm))\n",
    "print(\"LSTM test r2:\", r2_score(y_true_lstm, y_pred_lstm))\n",
    "\n",
    "\n",
    "# step: train and evaluate transformer model\n",
    "\n",
    "trf = TransformerRegressor(n_features=F, d_model=128, nhead=4, num_layers=2, dim_ff=256, dropout=0.1, max_len=512)\n",
    "trf = train_model(trf, train_loader, val_loader, device, epochs=EPOCHS, lr=7e-4, weight_decay=1e-4, patience=5)\n",
    "y_true_trf, y_pred_trf = predict_torch(trf, test_loader, device)\n",
    "print(\"\\nTransformer test mae:\", mean_absolute_error(y_true_trf, y_pred_trf))\n",
    "print(\"Transformer test rmse:\", rmse(y_true_trf, y_pred_trf))\n",
    "print(\"Transformer test r2:\", r2_score(y_true_trf, y_pred_trf))\n",
    "\n",
    "\n",
    "# step: permutation importance for sequence models (lstm + transformer)\n",
    "\n",
    "def perm_importance_sequence(\n",
    "    model,\n",
    "    X_seq,           # (N, T, F) scaled\n",
    "    y_true,          # (N,)\n",
    "    feature_names,   # list of len F\n",
    "    device,\n",
    "    batch_size=1024,\n",
    "    n_repeats=3,\n",
    "    shuffle_over_time=True,\n",
    "    seed=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns a list of (feature, mean_delta_rmse, std_delta_rmse) sorted desc.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    base_loader = DataLoader(SeqDataset(X_seq, y_true.astype(np.float32)), batch_size=batch_size, shuffle=False)\n",
    "    yt, yp = predict_torch(model, base_loader, device)\n",
    "    base_rmse = rmse(yt, yp)\n",
    "\n",
    "    deltas = np.zeros((len(feature_names), n_repeats), dtype=np.float64)\n",
    "\n",
    "    for j in range(len(feature_names)):\n",
    "        for r in range(n_repeats):\n",
    "            Xp = X_seq.copy()\n",
    "\n",
    "            if shuffle_over_time:\n",
    "                flat = Xp[:, :, j].reshape(-1)\n",
    "                perm = rng.permutation(flat.shape[0])\n",
    "                Xp[:, :, j] = flat[perm].reshape(Xp.shape[0], Xp.shape[1])\n",
    "            else:\n",
    "                perm = rng.permutation(Xp.shape[0])\n",
    "                Xp[:, :, j] = Xp[perm, :, j]\n",
    "\n",
    "            loader = DataLoader(SeqDataset(Xp, y_true.astype(np.float32)), batch_size=batch_size, shuffle=False)\n",
    "            yt2, yp2 = predict_torch(model, loader, device)\n",
    "            deltas[j, r] = rmse(yt2, yp2) - base_rmse\n",
    "\n",
    "    rows = []\n",
    "    for j, name in enumerate(feature_names):\n",
    "        rows.append((name, float(deltas[j].mean()), float(deltas[j].std(ddof=1) if n_repeats > 1 else 0.0)))\n",
    "\n",
    "    rows.sort(key=lambda x: x[1], reverse=True)\n",
    "    return base_rmse, rows\n",
    "\n",
    "# compute and print permutation importance for lstm\n",
    "lstm_base_rmse, lstm_pi = perm_importance_sequence(\n",
    "    model=lstm,\n",
    "    X_seq=Xte_seq,\n",
    "    y_true=yte_seq,\n",
    "    feature_names=list(best_feats),\n",
    "    device=device,\n",
    "    batch_size=1024,\n",
    "    n_repeats=3,\n",
    "    shuffle_over_time=True,\n",
    "    seed=0,\n",
    ")\n",
    "print(\"\\nLSTM permutation importance (delta RMSE vs baseline RMSE={:.6f})\".format(lstm_base_rmse))\n",
    "for feat, mean_d, std_d in lstm_pi[:20]:\n",
    "    print(f\"  {feat:>30s}  +{mean_d:.6f}  (std {std_d:.6f})\")\n",
    "\n",
    "# compute and print permutation importance for transformer\n",
    "trf_base_rmse, trf_pi = perm_importance_sequence(\n",
    "    model=trf,\n",
    "    X_seq=Xte_seq,\n",
    "    y_true=yte_seq,\n",
    "    feature_names=list(best_feats),\n",
    "    device=device,\n",
    "    batch_size=1024,\n",
    "    n_repeats=3,\n",
    "    shuffle_over_time=True,\n",
    "    seed=1,\n",
    ")\n",
    "print(\"\\nTransformer permutation importance (delta RMSE vs baseline RMSE={:.6f})\".format(trf_base_rmse))\n",
    "for feat, mean_d, std_d in trf_pi[:20]:\n",
    "    print(f\"  {feat:>30s}  +{mean_d:.6f}  (std {std_d:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebe1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "use_perm = \"shap_exp\" in globals()\n",
    "X_show = ex.copy() if use_perm else Xte.copy()\n",
    "sv = shap_exp.values if use_perm else shap_values\n",
    "\n",
    "j = list(X_show.columns).index(\"sent_surprise\")\n",
    "\n",
    "df = X_show.copy()\n",
    "df[\"shap_sent\"] = sv[:, j]\n",
    "df[\"vol_bucket\"] = pd.qcut(df[\"vol_z\"], q=3, labels=[\"low_volz\",\"mid_volz\",\"high_volz\"])\n",
    "df[\"sent_bin\"] = pd.qcut(df[\"sent_surprise\"], q=8, duplicates=\"drop\")\n",
    "\n",
    "out = (df.groupby([\"vol_bucket\",\"sent_bin\"], observed=True)[\"shap_sent\"]\n",
    "         .mean()\n",
    "         .reset_index())\n",
    "\n",
    "display(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856ee0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_plot = out.copy()\n",
    "\n",
    "# convert interval bins into numeric midpoints for plotting.\n",
    "df_plot[\"sent_mid\"] = df_plot[\"sent_bin\"].apply(lambda iv: float(iv.mid))\n",
    "\n",
    "# map vol buckets to a stable order.\n",
    "order = [\"low_volz\", \"mid_volz\", \"high_volz\"]\n",
    "df_plot[\"vol_bucket\"] = pd.Categorical(df_plot[\"vol_bucket\"], categories=order, ordered=True)\n",
    "df_plot = df_plot.sort_values([\"vol_bucket\", \"sent_mid\"])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for vb in order:\n",
    "    sub = df_plot[df_plot[\"vol_bucket\"] == vb]\n",
    "    plt.plot(sub[\"sent_mid\"], sub[\"shap_sent\"], marker=\"o\", label=vb)\n",
    "\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.title(\"line plot: sentiment surprise vs shap impact, split by vol_z\")\n",
    "plt.xlabel(\"sent_surprise (bin midpoint)\")\n",
    "plt.ylabel(\"mean shap(sent_surprise)\")\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f29801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "slopes = []\n",
    "for vb in [\"low_volz\", \"mid_volz\", \"high_volz\"]:\n",
    "    sub = df_plot[df_plot[\"vol_bucket\"] == vb]\n",
    "    x = sub[\"sent_mid\"].to_numpy()\n",
    "    y = sub[\"shap_sent\"].to_numpy()\n",
    "    m = float(np.polyfit(x, y, 1)[0])\n",
    "    slopes.append({\"vol_bucket\": vb, \"slope_shap_per_sent\": m})\n",
    "\n",
    "slopes_df = pd.DataFrame(slopes)\n",
    "display(slopes_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c441a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "slopes_df = pd.DataFrame({\n",
    "    \"vol_bucket\": [\"low_volz\",\"mid_volz\",\"high_volz\"],\n",
    "    \"slope_shap_per_sent\": [-0.000144, -0.000263, -0.000229]\n",
    "})\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(slopes_df[\"vol_bucket\"], slopes_df[\"slope_shap_per_sent\"], marker=\"o\")\n",
    "plt.axhline(0, linewidth=1)\n",
    "plt.title(\"line plot: slope of shap(sent_surprise) by volume bucket\")\n",
    "plt.ylabel(\"slope (shap per unit sentiment)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710c7b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# compute rmse with sqrt of mse.\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# fit xgb and return metrics.\n",
    "def fit_eval_xgb(Xtr, ytr, Xte, yte, seed=0):\n",
    "    base = float(np.mean(ytr))\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=800,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.03,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_lambda=1.0,\n",
    "        base_score=base,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    model.fit(Xtr, ytr)\n",
    "    pred = model.predict(Xte)\n",
    "    return {\n",
    "        \"mae\": mean_absolute_error(yte, pred),\n",
    "        \"rmse\": rmse(yte, pred),\n",
    "        \"r2\": r2_score(yte, pred),\n",
    "    }\n",
    "\n",
    "# pick your best feature set.\n",
    "best_feats = feature_sets[\"plus_missingness\"]\n",
    "\n",
    "Xtr = train[best_feats].copy()\n",
    "ytr = train[\"target_abs_logret\"].astype(float).to_numpy()\n",
    "Xte = test[best_feats].copy()\n",
    "yte = test[\"target_abs_logret\"].astype(float).to_numpy()\n",
    "\n",
    "# real model performance.\n",
    "real_metrics = fit_eval_xgb(Xtr, ytr, Xte, yte, seed=0)\n",
    "print(\"real:\", real_metrics)\n",
    "\n",
    "# build placebo by shuffling sentiment features within each ticker.\n",
    "sent_cols = [c for c in best_feats if \"sent\" in c]\n",
    "print(\"shuffling sentiment cols:\", sent_cols)\n",
    "\n",
    "train_pl = train.copy()\n",
    "test_pl = test.copy()\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "for c in sent_cols:\n",
    "    train_pl[c] = train_pl.groupby(\"ticker\")[c].transform(lambda s: rng.permutation(s.to_numpy()))\n",
    "    test_pl[c] = test_pl.groupby(\"ticker\")[c].transform(lambda s: rng.permutation(s.to_numpy()))\n",
    "\n",
    "Xtr_pl = train_pl[best_feats].copy()\n",
    "Xte_pl = test_pl[best_feats].copy()\n",
    "\n",
    "placebo_metrics = fit_eval_xgb(Xtr_pl, ytr, Xte_pl, yte, seed=0)\n",
    "print(\"placebo:\", placebo_metrics)\n",
    "\n",
    "# run multiple placebo shuffles to get a small null distribution.\n",
    "placebo_runs = []\n",
    "for k in range(30):\n",
    "    rng = np.random.default_rng(100 + k)\n",
    "    trp = train.copy()\n",
    "    tep = test.copy()\n",
    "    for c in sent_cols:\n",
    "        trp[c] = trp.groupby(\"ticker\")[c].transform(lambda s: rng.permutation(s.to_numpy()))\n",
    "        tep[c] = tep.groupby(\"ticker\")[c].transform(lambda s: rng.permutation(s.to_numpy()))\n",
    "\n",
    "    m = fit_eval_xgb(trp[best_feats], ytr, tep[best_feats], yte, seed=100 + k)\n",
    "    m[\"run\"] = k\n",
    "    placebo_runs.append(m)\n",
    "\n",
    "placebo_df = pd.DataFrame(placebo_runs)\n",
    "display(placebo_df.describe())\n",
    "\n",
    "# compare real to placebo in a simple table.\n",
    "summary = pd.DataFrame([\n",
    "    {\"setting\": \"real\", **real_metrics},\n",
    "    {\"setting\": \"placebo_mean\", \"mae\": placebo_df[\"mae\"].mean(), \"rmse\": placebo_df[\"rmse\"].mean(), \"r2\": placebo_df[\"r2\"].mean()},\n",
    "    {\"setting\": \"placebo_std\", \"mae\": placebo_df[\"mae\"].std(), \"rmse\": placebo_df[\"rmse\"].std(), \"r2\": placebo_df[\"r2\"].std()},\n",
    "])\n",
    "display(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab216fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# compute rmse with sqrt of mse.\n",
    "def rmse(y_true, y_pred) -> float:\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "# rolling window evaluation across time.\n",
    "def rolling_eval(panel_ml: pd.DataFrame, feats, target_col=\"target_abs_logret\",\n",
    "                 train_years=8, test_months=6, min_train_rows=2000, seed=0):\n",
    "\n",
    "    df = panel_ml.sort_values(\"date\").copy()\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"]).dt.floor(\"D\")\n",
    "\n",
    "    start = df[\"date\"].min()\n",
    "    end = df[\"date\"].max()\n",
    "\n",
    "    # start after we have enough training history.\n",
    "    cursor = start + pd.DateOffset(years=train_years)\n",
    "    rows = []\n",
    "\n",
    "    while cursor + pd.DateOffset(months=test_months) <= end:\n",
    "        train_end = cursor\n",
    "        test_end = cursor + pd.DateOffset(months=test_months)\n",
    "\n",
    "        tr = df[df[\"date\"] < train_end].copy()\n",
    "        te = df[(df[\"date\"] >= train_end) & (df[\"date\"] < test_end)].copy()\n",
    "\n",
    "        if len(tr) < min_train_rows or len(te) < 500:\n",
    "            cursor = cursor + pd.DateOffset(months=test_months)\n",
    "            continue\n",
    "\n",
    "        Xtr = tr[feats]\n",
    "        ytr = tr[target_col].astype(float).to_numpy()\n",
    "        Xte = te[feats]\n",
    "        yte = te[target_col].astype(float).to_numpy()\n",
    "\n",
    "        base = float(np.mean(ytr))\n",
    "        model = xgb.XGBRegressor(\n",
    "            n_estimators=800,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.03,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            reg_lambda=1.0,\n",
    "            base_score=base,\n",
    "            random_state=seed,\n",
    "        )\n",
    "        model.fit(Xtr, ytr)\n",
    "        pred = model.predict(Xte)\n",
    "\n",
    "        rows.append({\n",
    "            \"train_end\": train_end.date(),\n",
    "            \"test_end\": test_end.date(),\n",
    "            \"train_rows\": len(tr),\n",
    "            \"test_rows\": len(te),\n",
    "            \"mae\": mean_absolute_error(yte, pred),\n",
    "            \"rmse\": rmse(yte, pred),\n",
    "            \"r2\": r2_score(yte, pred),\n",
    "        })\n",
    "\n",
    "        cursor = cursor + pd.DateOffset(months=test_months)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# run rolling evaluation for two competing feature sets.\n",
    "feats_price = feature_sets[\"price_only\"]\n",
    "feats_full = feature_sets[\"plus_missingness\"]\n",
    "\n",
    "roll_price = rolling_eval(panel_ml, feats_price, seed=0)\n",
    "roll_full = rolling_eval(panel_ml, feats_full, seed=0)\n",
    "\n",
    "roll_price[\"model\"] = \"price_only\"\n",
    "roll_full[\"model\"] = \"plus_missingness\"\n",
    "roll = pd.concat([roll_price, roll_full], ignore_index=True)\n",
    "\n",
    "display(roll.head(10))\n",
    "display(roll.groupby(\"model\")[[\"mae\",\"rmse\",\"r2\"]].agg([\"mean\",\"median\",\"std\"]))\n",
    "\n",
    "# compute per-window deltas (full minus price).\n",
    "merged = roll_price.merge(roll_full, on=[\"train_end\",\"test_end\"], suffixes=(\"_price\",\"_full\"))\n",
    "merged[\"rmse_delta_full_minus_price\"] = merged[\"rmse_full\"] - merged[\"rmse_price\"]\n",
    "merged[\"mae_delta_full_minus_price\"] = merged[\"mae_full\"] - merged[\"mae_price\"]\n",
    "merged[\"r2_delta_full_minus_price\"] = merged[\"r2_full\"] - merged[\"r2_price\"]\n",
    "\n",
    "display(merged[[\"train_end\",\"test_end\",\"rmse_price\",\"rmse_full\",\"rmse_delta_full_minus_price\",\n",
    "                \"mae_price\",\"mae_full\",\"mae_delta_full_minus_price\",\n",
    "                \"r2_price\",\"r2_full\",\"r2_delta_full_minus_price\"]].head(12))\n",
    "\n",
    "print(\"delta rmse mean:\", merged[\"rmse_delta_full_minus_price\"].mean())\n",
    "print(\"delta rmse median:\", merged[\"rmse_delta_full_minus_price\"].median())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
